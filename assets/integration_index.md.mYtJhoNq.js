import{_ as a,c as s,o as i,a3 as t}from"./chunks/framework.CGTGJaqa.js";const g=JSON.parse('{"title":"大模型集成","description":"","frontmatter":{},"headers":[],"relativePath":"integration/index.md","filePath":"integration/index.md"}'),e={name:"integration/index.md"},h=t(`<h1 id="大模型集成" tabindex="-1">大模型集成 <a class="header-anchor" href="#大模型集成" aria-label="Permalink to &quot;大模型集成&quot;">​</a></h1><h2 id="概述" tabindex="-1">概述 <a class="header-anchor" href="#概述" aria-label="Permalink to &quot;概述&quot;">​</a></h2><p>2022 年 11 月 30 日，OpenAI 推出 ChatGPT，令人没想到的是，这个对话模型在 AI 圈掀起一股又一股讨论狂潮。国内诸多互联网巨头、科技企业及研究机构纷纷宣布在 AIGC 的领域进行产业布局，国产大模型进入集中发布期，市场上已拉开“百模大战”的序幕，据公开信息，截至 2023 年 10 月，中国累计发布 200 余个大模型。</p><p>可以说，大模型的突破和 AIGC 的兴起为企业实现产品/流程的革新提供先进生产工具，引领企业和产业迈入智能创新的新时代。同时，大模型等 AIGC 的蓬勃发展，也为我们带来了新的机遇和挑战。</p><p>在当今的技术环境下，大型语言模型（LLM）的集成和部署对于提升应用程序的智能化水平至关重要。FastChat 作为一款先进的分布式的多模型服务系统，提供了一个理想的环境，用于快速部署和集成 LLM。</p><p>本方案将详细介绍如何利用 FastChat 的架构和工具，高效地集成和部署大型语言模型，以增强用户体验和提高业务效率。</p><h2 id="方案优势" tabindex="-1">方案优势 <a class="header-anchor" href="#方案优势" aria-label="Permalink to &quot;方案优势&quot;">​</a></h2><ul><li>🚀 简单易用：FastChat 提供开箱即用的集成和部署工具，用户只需简单配置即可快速部署和集成大模型。</li><li>👻 分布式服务：带有 WebUI 和与 OpenAI 兼容的 RESTful API 的分布式多模型服务系统。</li></ul><h2 id="安装-fastchat" tabindex="-1">安装 FastChat <a class="header-anchor" href="#安装-fastchat" aria-label="Permalink to &quot;安装 FastChat&quot;">​</a></h2><p>FastChat 是一个基于 Python 的开源项目。安装方式有两种，一种是通过 Pip 进行安装，一种是通过源码安装，由于源码的方式比较复杂，这里我们使用 Pip 的方式来安装。</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">pip</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> fastchat</span></span></code></pre></div><h2 id="llm-下载" tabindex="-1">LLM 下载 <a class="header-anchor" href="#llm-下载" aria-label="Permalink to &quot;LLM 下载&quot;">​</a></h2><p>LLM 的下载我们可以通过 <a href="https://www.huggingface.co" target="_blank" rel="noreferrer">HuggingFace</a> 或 <a href="https://www.modelscope.cn" target="_blank" rel="noreferrer">modelscope</a> 上进行下载，我们演示 Qwen-7B-Chat 的下载</p><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 开启大文件下载</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> lfs</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> install</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 下载模型</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">git</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> lfs</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> clone</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> https://www.modelscope.cn/qwen/Qwen-7B-Chat</span></span></code></pre></div><blockquote><p>注意：国内环境下，使用 HuggingFace 下载模型可能会比较慢，建议使用 Modelscope 下载。</p></blockquote><h2 id="api-部署" tabindex="-1">API 部署 <a class="header-anchor" href="#api-部署" aria-label="Permalink to &quot;API 部署&quot;">​</a></h2><p>FastChat 提供了与 OpenAI 兼容的 RESTful API，我们可以通过该 API 来进行大模型的部署和调用。部署 API 服务分三步:</p><ul><li>第一步：启动 controller 服务</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> fastchat.serve.controller</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --host</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.0</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">.0.0</span></span></code></pre></div><ul><li>第二步：启动 model_worker 服务</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> fastchat.serve.model_worker</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --host</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.0</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">.0.0</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">--num-gpus </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> \\</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">--model-path </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">./Qwen-7B-Chat</span></span></code></pre></div><blockquote><p>注意：这里需要指定模型路径，即上一步下载的模型路径。</p></blockquote><ul><li>第三步：启动 api_server 服务</li></ul><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">python</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> -m</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> fastchat.serve.openai_api_server</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> --host</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.0</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">.0.0</span></span></code></pre></div><blockquote><p>注意：服务启动后，默认端口是 8000，可以通过--port 参数来修改端口，在浏览器中访问<code>http://127.0.0.1:8000/docs</code>，这是我们最终要用的 LLM API 服务，它的接口跟 OpenAI 的接口是兼容的，不仅可以推理，还可以进行 Embedding 操作。</p></blockquote><h2 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h2><p>其实 FastChat 的功能非常强大，今天介绍只是 API 服务功能，如果你对 FastChat 感兴趣的话，可以去<a href="https://github.com/lm-sys/FastChat" target="_blank" rel="noreferrer">官方仓库</a>查看更多的信息。</p><h2 id="参考资料" tabindex="-1">参考资料 <a class="header-anchor" href="#参考资料" aria-label="Permalink to &quot;参考资料&quot;">​</a></h2><ul><li><a href="https://github.com/lm-sys/FastChat" target="_blank" rel="noreferrer">https://github.com/lm-sys/FastChat</a></li></ul>`,29),l=[h];function n(p,r,o,k,d,c){return i(),s("div",null,l)}const C=a(e,[["render",n]]);export{g as __pageData,C as default};
